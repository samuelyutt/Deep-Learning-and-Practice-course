import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.utils import save_image

from dataset import CLEVRDataset
from models import Generator, Discriminator
from evaluator import evaluation_model

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
latent_size, cond_size = 100, 1024
checkpoint_dir = './checkpoint'
out_dir = './out'


def evaluate(G, test_data, evaluator=evaluation_model()):
    test_cond, test_z = test_data
    G.to(device)
    G.eval()
    with torch.no_grad():
        gen_imgs = G(test_z.view(-1, latent_size, 1, 1), test_cond)
    # gen_imgs = gen_imgs.add(1).div(2).mul(255).clamp(0, 255)
    acc = evaluator.eval(gen_imgs, test_cond)
    return acc, gen_imgs


def train(G, D, train_loader, test_data, lr=0.0002, epochs=200, criterion=nn.BCELoss()):
    print(f'Training on device {device}')

    # Initialize training
    optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))
    G.to(device)
    D.to(device)
    all_test_acc, best_test_acc = [], -1.0

    for e in range(epochs):
        G.train()
        D.train()
        total_loss_G, total_loss_D = 0.0, 0.0

        for imgs, conds in train_loader:
            imgs = imgs.to(device)
            conds = conds.to(device)
            batch_size = len(conds)
            real = torch.ones(batch_size).to(device)
            fake = torch.zeros(batch_size).to(device)

            # Train D
            optimizer_D.zero_grad()

            # Feed D with real data
            pred_D = D(imgs, conds)
            real_loss_D = criterion(pred_D, real)

            # Feed D with fake data generated by G
            z = torch.randn(batch_size, latent_size).to(device).view(-1, latent_size, 1, 1)
            gen_imgs = G(z, conds)
            pred_D = D(gen_imgs, conds)
            fake_loss_D = criterion(pred_D, fake)

            # Back propagation of D
            loss_D = real_loss_D + fake_loss_D
            # D.zero_grad()
            loss_D.backward()
            optimizer_D.step()

            # Train G
            for _ in range(4):
                optimizer_G.zero_grad()

                # Feed D with fake data generated by G
                z = torch.randn(batch_size, latent_size).to(device).view(-1, latent_size, 1, 1)
                gen_imgs = G(z, conds)
                pred_D = D(gen_imgs, conds)
                loss_G = criterion(pred_D, real)

                # Back propagation of G
                # G.zero_grad()
                loss_G.backward()
                optimizer_G.step()

            total_loss_G += loss_G.item()
            total_loss_D += loss_D.item()

        # Statistics
        total_loss_G /= len(train_loader.dataset)
        total_loss_D /= len(train_loader.dataset)

        # Evaluate and logs
        test_acc, test_gen_imgs = evaluate(G, test_data)
        all_test_acc.append(test_acc)
        print(f'Epoch {e + 1} G Loss: {total_loss_G} D Loss: {total_loss_D} Test Acc: {test_acc}')

        # Save generated test images and checkpoint
        G.save(f'{checkpoint_dir}/G_{e}.pt')
        D.save(f'{checkpoint_dir}/D_{e}.pt')
        save_image(test_gen_imgs, f'{out_dir}/{e}.png', nrow=8, normalize=True)
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            G.save(f'{checkpoint_dir}/G_BEST.pt')
            D.save(f'{checkpoint_dir}/D_BEST.pt')
            save_image(test_gen_imgs, f'{out_dir}/BEST.png', nrow=8, normalize=True)


def get_test_data(filename='test.json'):
    with open(f'../data/objects.json') as f:
        objects = json.load(f)
    with open(f'../data/{filename}') as f:
        labels = json.load(f)
    test_one_hot_labels = torch.zeros((len(labels), len(objects)), device=device)
    for i, label in enumerate(labels):
        for obj in label:
            test_one_hot_labels[i][objects[obj]] = 1

    test_z = torch.randn(len(test_one_hot_labels), latent_size, device=device)

    return test_one_hot_labels, test_z


def inference(G):
    # test.json
    test_file = 'test.json'
    seed, ckpt_epoch = 279, 196
    torch.random.manual_seed(seed)
    test_data = get_test_data(test_file)
    G.load(f'{checkpoint_dir}/G_{ckpt_epoch}.pt', device=device)
    test_acc, test_gen_imgs = evaluate(G, test_data)
    print(f'{test_file} Test Acc: {test_acc}')
    save_image(test_gen_imgs, f'{out_dir}/{test_file}_inference.png', nrow=8, normalize=True)

    # new_test.json
    test_file = 'new_test.json'
    seed, ckpt_epoch = 689, 177
    torch.random.manual_seed(seed)
    test_data = get_test_data(test_file)
    G.load(f'{checkpoint_dir}/G_{ckpt_epoch}.pt', device=device)
    test_acc, test_gen_imgs = evaluate(G, test_data)
    print(f'{test_file} Test Acc: {test_acc}')
    save_image(test_gen_imgs, f'{out_dir}/{test_file}_inference.png', nrow=8, normalize=True)


def test_epoch_and_seed():
    test_files = ['test.json', 'new_test.json']
    for test_file in test_files:
        best_seed, best_epoch, best_test_acc = 0, 0, -1.0
        for seed in range(3000):
            torch.random.manual_seed(seed)
            test_data = get_test_data(test_file)

            for e in range(50, 200):
                G = Generator(z_size=latent_size, c_size=cond_size)
                G.load(f'./history/3_BEST/checkpoint/G_{e}.pt', device=device)
                test_acc, test_gen_imgs = evaluate(G, test_data)

                if test_acc > best_test_acc:
                    best_seed, best_epoch = seed, e
                    best_test_acc = test_acc
                    print(f'{test_file} Current BEST (seed, epoch) ({best_seed}, {best_epoch}) BEST Test Acc: {best_test_acc}')

        print(f'==== {test_file} Final BEST (seed, epoch) ({best_seed}, {best_epoch}) BEST Test Acc: {best_test_acc} ====')
        print()


def main():
    # Training
    # train_loader = DataLoader(CLEVRDataset(), batch_size=128, num_workers=4)
    # test_data = get_test_data()
    # G = Generator(z_size=latent_size, c_size=cond_size)
    # D = Discriminator(img_shape=(64, 64))
    # train(G, D, train_loader, test_data)

    # Inference
    G = Generator(z_size=latent_size, c_size=cond_size)
    inference(G)

    # test_epoch_and_seed()


if __name__ == '__main__':
    main()
